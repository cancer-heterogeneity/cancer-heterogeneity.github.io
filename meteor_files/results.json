{
    "Results - Single Phase with Multiple Task(335)": {
        "magrichardtest-2227": {
            "fact_sheet_answers": {
                "submission_id": "DT2"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "1.7661902996",
            "MT1_r_WIC(614)-Accuracy_sd": "0.2994314161",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "",
            "MT2_r_WNM(615)-Accuracy_sd": "",
            "MT2_r_WNM(615)-Time": "",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "",
            "MT7_Cibersort_abs(619)-Time": "",
            "MT8_Cibersort(620)-Accuracy_mean": "",
            "MT8_Cibersort(620)-Accuracy_sd": "",
            "MT8_Cibersort(620)-Time": "",
            "MT9_EPIC(582)-Accuracy_mean": "",
            "MT9_EPIC(582)-Accuracy_sd": "",
            "MT9_EPIC(582)-Time": "",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "",
            "MT11_QuantiSeq(622)-Accuracy_sd": "",
            "MT11_QuantiSeq(622)-Time": "",
            "MT13_Xcells(623)-Accuracy_mean": "",
            "MT13_Xcells(623)-Accuracy_sd": "",
            "MT13_Xcells(623)-Time": ""
        },
        "magrichardtest-2224": {
            "fact_sheet_answers": {
                "submission_id": "DT1"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "2.2845803622",
            "MT1_r_WIC(614)-Accuracy_sd": "0.4530999755",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "3.4252710134",
            "MT2_r_WNM(615)-Accuracy_sd": "0.3388631999",
            "MT2_r_WNM(615)-Time": "0.0000000000",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "",
            "MT7_Cibersort_abs(619)-Time": "",
            "MT8_Cibersort(620)-Accuracy_mean": "",
            "MT8_Cibersort(620)-Accuracy_sd": "",
            "MT8_Cibersort(620)-Time": "",
            "MT9_EPIC(582)-Accuracy_mean": "",
            "MT9_EPIC(582)-Accuracy_sd": "",
            "MT9_EPIC(582)-Time": "",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "",
            "MT11_QuantiSeq(622)-Accuracy_sd": "",
            "MT11_QuantiSeq(622)-Time": "",
            "MT13_Xcells(623)-Accuracy_mean": "",
            "MT13_Xcells(623)-Accuracy_sd": "",
            "MT13_Xcells(623)-Time": ""
        },
        "magrichardtest-2235": {
            "fact_sheet_answers": {
                "submission_id": "DT5"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "1.3684632212",
            "MT1_r_WIC(614)-Accuracy_sd": "0.0000000000",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "",
            "MT2_r_WNM(615)-Accuracy_sd": "",
            "MT2_r_WNM(615)-Time": "",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "1.6375063351",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "0.0000000000",
            "MT7_Cibersort_abs(619)-Time": "0.0000000000",
            "MT8_Cibersort(620)-Accuracy_mean": "1.4751147006",
            "MT8_Cibersort(620)-Accuracy_sd": "0.0000000000",
            "MT8_Cibersort(620)-Time": "0.0000000000",
            "MT9_EPIC(582)-Accuracy_mean": "1.1752592526",
            "MT9_EPIC(582)-Accuracy_sd": "0.0000000000",
            "MT9_EPIC(582)-Time": "0.0000000000",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "2.1540611089",
            "MT11_QuantiSeq(622)-Accuracy_sd": "0.0000000000",
            "MT11_QuantiSeq(622)-Time": "0.0000000000",
            "MT13_Xcells(623)-Accuracy_mean": "0.9577968319",
            "MT13_Xcells(623)-Accuracy_sd": "0.0000000000",
            "MT13_Xcells(623)-Time": "0.0000000000"
        },
        "magrichardtest-2242": {
            "fact_sheet_answers": {
                "submission_id": "DT4"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "1.7107604445",
            "MT1_r_WIC(614)-Accuracy_sd": "0.0000000000",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "",
            "MT2_r_WNM(615)-Accuracy_sd": "",
            "MT2_r_WNM(615)-Time": "",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "1.6375063351",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "0.0000000000",
            "MT7_Cibersort_abs(619)-Time": "0.0000000000",
            "MT8_Cibersort(620)-Accuracy_mean": "1.4751147006",
            "MT8_Cibersort(620)-Accuracy_sd": "0.0000000000",
            "MT8_Cibersort(620)-Time": "0.0000000000",
            "MT9_EPIC(582)-Accuracy_mean": "0.8653365894",
            "MT9_EPIC(582)-Accuracy_sd": "0.0000000000",
            "MT9_EPIC(582)-Time": "0.0000000000",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "2.1540611089",
            "MT11_QuantiSeq(622)-Accuracy_sd": "0.0000000000",
            "MT11_QuantiSeq(622)-Time": "0.0000000000",
            "MT13_Xcells(623)-Accuracy_mean": "0.9577968319",
            "MT13_Xcells(623)-Accuracy_sd": "0.0000000000",
            "MT13_Xcells(623)-Time": "0.0000000000"
        },
        "magrichardtest-2234": {
            "fact_sheet_answers": {
                "submission_id": "DT12"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "",
            "MT1_r_WIC(614)-Accuracy_sd": "",
            "MT1_r_WIC(614)-Time": "",
            "MT2_r_WNM(615)-Accuracy_mean": "",
            "MT2_r_WNM(615)-Accuracy_sd": "",
            "MT2_r_WNM(615)-Time": "",
            "MT3_m_EDEC(616)-Accuracy_mean": "1.9789139758",
            "MT3_m_EDEC(616)-Accuracy_sd": "0.4548928001",
            "MT3_m_EDEC(616)-Time": "0.0000000000",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "",
            "MT7_Cibersort_abs(619)-Time": "",
            "MT8_Cibersort(620)-Accuracy_mean": "",
            "MT8_Cibersort(620)-Accuracy_sd": "",
            "MT8_Cibersort(620)-Time": "",
            "MT9_EPIC(582)-Accuracy_mean": "",
            "MT9_EPIC(582)-Accuracy_sd": "",
            "MT9_EPIC(582)-Time": "",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "",
            "MT11_QuantiSeq(622)-Accuracy_sd": "",
            "MT11_QuantiSeq(622)-Time": "",
            "MT13_Xcells(623)-Accuracy_mean": "",
            "MT13_Xcells(623)-Accuracy_sd": "",
            "MT13_Xcells(623)-Time": ""
        },
        "magrichardtest-2233": {
            "fact_sheet_answers": {
                "submission_id": "DT9"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "0.8052700626",
            "MT1_r_WIC(614)-Accuracy_sd": "0.0000000000",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "",
            "MT2_r_WNM(615)-Accuracy_sd": "",
            "MT2_r_WNM(615)-Time": "",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "",
            "MT7_Cibersort_abs(619)-Time": "",
            "MT8_Cibersort(620)-Accuracy_mean": "",
            "MT8_Cibersort(620)-Accuracy_sd": "",
            "MT8_Cibersort(620)-Time": "",
            "MT9_EPIC(582)-Accuracy_mean": "",
            "MT9_EPIC(582)-Accuracy_sd": "",
            "MT9_EPIC(582)-Time": "",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "",
            "MT11_QuantiSeq(622)-Accuracy_sd": "",
            "MT11_QuantiSeq(622)-Time": "",
            "MT13_Xcells(623)-Accuracy_mean": "",
            "MT13_Xcells(623)-Accuracy_sd": "",
            "MT13_Xcells(623)-Time": ""
        },
        "magrichardtest-2230": {
            "fact_sheet_answers": {
                "submission_id": "DT8"
            },
            "MT1_r_WIC(614)-Accuracy_mean": "0.3748867963",
            "MT1_r_WIC(614)-Accuracy_sd": "0.0000000000",
            "MT1_r_WIC(614)-Time": "0.0000000000",
            "MT2_r_WNM(615)-Accuracy_mean": "0.2674850720",
            "MT2_r_WNM(615)-Accuracy_sd": "0.0000000000",
            "MT2_r_WNM(615)-Time": "0.0000000000",
            "MT3_m_EDEC(616)-Accuracy_mean": "",
            "MT3_m_EDEC(616)-Accuracy_sd": "",
            "MT3_m_EDEC(616)-Time": "",
            "MT5_m_WIC(617)-Accuracy_mean": "",
            "MT5_m_WIC(617)-Accuracy_sd": "",
            "MT5_m_WIC(617)-Time": "",
            "MT6_EpiDISH(618)-Accuracy_mean": "",
            "MT6_EpiDISH(618)-Accuracy_sd": "",
            "MT6_EpiDISH(618)-Time": "",
            "MT7_Cibersort_abs(619)-Accuracy_mean": "",
            "MT7_Cibersort_abs(619)-Accuracy_sd": "",
            "MT7_Cibersort_abs(619)-Time": "",
            "MT8_Cibersort(620)-Accuracy_mean": "",
            "MT8_Cibersort(620)-Accuracy_sd": "",
            "MT8_Cibersort(620)-Time": "",
            "MT9_EPIC(582)-Accuracy_mean": "",
            "MT9_EPIC(582)-Accuracy_sd": "",
            "MT9_EPIC(582)-Time": "",
            "MT10_MCP_counter(621)-Accuracy_mean": "",
            "MT10_MCP_counter(621)-Accuracy_sd": "",
            "MT10_MCP_counter(621)-Time": "",
            "MT11_QuantiSeq(622)-Accuracy_mean": "",
            "MT11_QuantiSeq(622)-Accuracy_sd": "",
            "MT11_QuantiSeq(622)-Time": "",
            "MT13_Xcells(623)-Accuracy_mean": "",
            "MT13_Xcells(623)-Accuracy_sd": "",
            "MT13_Xcells(623)-Time": ""
        }
    }
}